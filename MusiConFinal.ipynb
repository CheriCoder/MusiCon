{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c159cd3-dfc3-4b7a-9cdd-87c799eefab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import mediapipe as mp \n",
    "from math import hypot \n",
    "import numpy as np \n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import pyautogui\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# Right Hand for Volume\n",
    "# Left Hand for Media Control\n",
    "\n",
    "# Open the default camera (index 0)\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# Initialize MediaPipe Hands module\n",
    "mpHands = mp.solutions.hands \n",
    "hands = mpHands.Hands(min_detection_confidence=0.75, min_tracking_confidence=0.75)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "# Get the audio devices and initialize volume control\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# Get the volume range\n",
    "volMin,volMax = volume.GetVolumeRange()[:2]\n",
    "\n",
    "# Initialize the variable to store the previous finger count\n",
    "prev_fingers = 0\n",
    "\n",
    "while True:\n",
    "    # Read frame from the camera\n",
    "    success,img = cap.read()\n",
    "    # Flip the frame horizontally\n",
    "    img = cv2.flip(img,1)\n",
    "    # Convert BGR image to RGB for processing with Mediapipe\n",
    "    imgRGB = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    # Process the frame to detect hands\n",
    "    results = hands.process(imgRGB)\n",
    "\n",
    "    # Lists to store landmark positions of left and right hands\n",
    "    left_lmList,right_lmList = [],[]\n",
    "    \n",
    "    # Process each hand in the frame\n",
    "    if results.multi_hand_landmarks and results.multi_handedness:\n",
    "        for i in range(len(results.multi_hand_landmarks)):\n",
    "            hand_landmarks = results.multi_hand_landmarks[i]\n",
    "            label = MessageToDict(results.multi_handedness[i])['classification'][0]['label']\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                h,w,_ = img.shape\n",
    "                # Store the landmark positions for left and right hands\n",
    "                if label == 'Left':\n",
    "                    left_lmList.append([int(lm.x*w),int(lm.y*h)])  \n",
    "                elif label == 'Right':\n",
    "                    right_lmList.append([int(lm.x*w),int(lm.y*h)])  \n",
    "    \n",
    "    # Process left hand for media control\n",
    "    if left_lmList:\n",
    "        # Count fingers\n",
    "        fingers = 0\n",
    "        if left_lmList:\n",
    "            if left_lmList[4][1] > left_lmList[3][1]:\n",
    "                fingers += 1\n",
    "            if left_lmList[8][1] > left_lmList[6][1]:\n",
    "                fingers += 1\n",
    "            if left_lmList[12][1] > left_lmList[10][1]:\n",
    "                fingers += 1\n",
    "            if left_lmList[16][1] > left_lmList[14][1]:\n",
    "                fingers += 1\n",
    "            if left_lmList[20][1] > left_lmList[18][1]:\n",
    "                fingers += 1\n",
    "                \n",
    "            # Perform media control actions based on finger count\n",
    "            if fingers == 1:\n",
    "                if prev_fingers != 1:\n",
    "                    # Simulate keyboard shortcut for previous track (Ctrl + B)\n",
    "                    pyautogui.hotkey('ctrl', 'b')\n",
    "            elif fingers == 2:\n",
    "                if prev_fingers != 2:\n",
    "                    # Simulate keyboard shortcut for play/pause (Ctrl + P)\n",
    "                    pyautogui.hotkey('ctrl', 'p')\n",
    "            elif fingers == 3:\n",
    "                if prev_fingers != 3:\n",
    "                    # Simulate keyboard shortcut for next track (Ctrl + F)\n",
    "                    pyautogui.hotkey('ctrl', 'f')\n",
    "            prev_fingers = fingers\n",
    "    \n",
    "    # Process right hand for volume control\n",
    "    if right_lmList:\n",
    "        x1,y1 = right_lmList[4][0],right_lmList[4][1]\n",
    "        x2,y2 = right_lmList[8][0],right_lmList[8][1]\n",
    "\n",
    "        cv2.line(img,(x1,y1),(x2,y2),(0,255,0),3)\n",
    "\n",
    "        length = hypot(x2-x1,y2-y1)\n",
    "\n",
    "        # Volume control based on the distance between thumb and index finger\n",
    "        if length < 50:\n",
    "            volume.SetMasterVolumeLevel(volMin, None)  # Minimum volume\n",
    "        elif length > 200:\n",
    "            volume.SetMasterVolumeLevel(volMax, None)  # Maximum volume\n",
    "        else:\n",
    "            vol = np.interp(length,[50,200],[volMin,volMax])\n",
    "            volume.SetMasterVolumeLevel(vol, None)\n",
    "        \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow('Image',img)\n",
    "    \n",
    "    # Check for exit key\n",
    "    if cv2.waitKey(1) & 0xff==ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
